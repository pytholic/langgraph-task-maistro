{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "# Connecting to the task_maistro Deployment\n",
        "\n",
        "## Deployment\n",
        "\n",
        "See `creating.ipynb` for the full build and launch walkthrough. In short:\n",
        "\n",
        "* The [LangGraph CLI](https://docs.langchain.com/langsmith/cli) packages `task_maistro` into a Docker image.\n",
        "* `docker-compose.yml` launches three containers:\n",
        "    * `langgraph-redis`: Redis for streaming pub/sub.\n",
        "    * `langgraph-postgres`: Postgres for persistent state and long-term store.\n",
        "    * `langgraph-api`: The `task-maistro` server image.\n",
        "\n",
        "```\n",
        "$ docker compose --env-file .env up\n",
        "```\n",
        "\n",
        "Once running, access the deployment through:\n",
        "\n",
        "* API: http://localhost:8123\n",
        "* Docs: http://localhost:8123/docs\n",
        "* LangGraph Studio: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123\n",
        "\n",
        "## Using the API\n",
        "\n",
        "LangGraph Server exposes [REST API endpoints](https://github.com/langchain-ai/agent-protocol) grouped into three areas:\n",
        "\n",
        "* **Runs**: Atomic agent executions\n",
        "* **Threads**: Multi-turn interactions and human-in-the-loop\n",
        "* **Store**: Long-term memory\n",
        "\n",
        "You can test any endpoint directly in the [interactive API docs](http://localhost:8123/docs#tag/thread-runs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## SDK\n",
        "\n",
        "The [LangGraph SDKs](https://docs.langchain.com/langsmith/sdk) (Python and JS) provide a developer-friendly interface to interact with the LangGraph Server API presented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph_sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Connect via SDK\n",
        "url_for_cli_deployment = \"http://localhost:8123\"\n",
        "client = get_client(url=url_for_cli_deployment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## Remote Graph\n",
        "\n",
        "If you are working in the LangGraph library, [Remote Graph](https://docs.langchain.com/langsmith/use-remote-graph) is also a useful way to connect directly to the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.pregel.remote import RemoteGraph\n",
        "from langchain_core.messages import convert_to_messages\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Connect via remote graph\n",
        "url = \"http://localhost:8123\"\n",
        "graph_name = \"task_maistro\" \n",
        "remote_graph = RemoteGraph(graph_name, url=url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## Runs\n",
        "\n",
        "A \"run\" represents a [single execution](https://github.com/langchain-ai/agent-protocol?tab=readme-ov-file#runs-atomic-agent-executions) of your graph. Each time a client makes a request:\n",
        "\n",
        "1. The HTTP worker generates a unique run ID\n",
        "2. This run and its results are stored in PostgreSQL\n",
        "3. You can query these runs to:\n",
        "   - Check their status\n",
        "   - Get their results\n",
        "   - Track execution history\n",
        "\n",
        "You can see a full set of How To guides for various types of runs [here](https://langchain-ai.github.io/langgraph/how-tos/#runs).\n",
        "\n",
        "Let's looks at a few of the interesting things we can do with runs.\n",
        "\n",
        "### Background Runs\n",
        "\n",
        "The LangGraph server supports two types of runs: \n",
        "\n",
        "* `Fire and forget` - Launch a run in the background, but don’t wait for it to finish\n",
        "* `Waiting on a reply (blocking or polling)` - Launch a run and wait/stream its output\n",
        "\n",
        "Background runs and polling are quite useful when working with long-running agents. \n",
        "\n",
        "Let's [see](https://docs.langchain.com/langsmith/background-run) how this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thread_id': '3619d2d0-f72d-403a-9bb2-2b5e0e2ca342',\n",
              " 'created_at': '2026-02-17T03:52:00.093043+00:00',\n",
              " 'updated_at': '2026-02-17T03:52:00.093043+00:00',\n",
              " 'metadata': {},\n",
              " 'config': {},\n",
              " 'error': None,\n",
              " 'status': 'idle',\n",
              " 'values': None,\n",
              " 'interrupts': {}}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a thread\n",
        "thread = await client.threads.create()\n",
        "thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Check any existing runs on a thread\n",
        "thread = await client.threads.create()\n",
        "runs = await client.runs.list(thread[\"thread_id\"])\n",
        "print(runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we've created some ToDos and saved them to my user_id\n",
        "user_input = \"Add a ToDo to finish booking travel to Hong Kong by end of next week. Also, add a ToDo to call parents back about Thanksgiving plans.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test\"}}\n",
        "graph_name = \"task_maistro\" \n",
        "run = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kick off a new thread and a new run\n",
        "thread = await client.threads.create()\n",
        "user_input = \"Give me a summary of all ToDos.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test\"}}\n",
        "graph_name = \"task_maistro\" \n",
        "run = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'run_id': '019c69bb-2241-7522-a8a6-85a779715c18', 'thread_id': '6e808947-648c-4cf7-9c78-0e3e393104fe', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'created_at': '2026-02-17T03:53:17.634747+00:00', 'updated_at': '2026-02-17T03:53:20.789154+00:00', 'status': 'success', 'metadata': {'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}, 'kwargs': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Give me a summary of all ToDos.', 'additional_kwargs': {}, 'response_metadata': {}}]}, 'config': {'configurable': {'thread_id': '6e808947-648c-4cf7-9c78-0e3e393104fe', 'graph_id': 'task_maistro', '__request_start_time_ms__': 1771300397633, 'langgraph_request_id': '4fd0df50-4039-44dd-bf4f-51c0fb6002d8', 'langgraph_auth_user': None, 'user_id': 'Test', 'langgraph_auth_user_id': '', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'langgraph_auth_permissions': [], '__after_seconds__': 0, 'run_id': '019c69bb-2241-7522-a8a6-85a779715c18'}, 'metadata': {'created_by': 'system', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}}, 'context': {'user_id': 'Test'}, 'command': None, 'stream_mode': ['values'], 'interrupt_before': None, 'interrupt_after': None, 'webhook': None, 'feedback_keys': None, 'temporary': False, 'subgraphs': False, 'resumable': False, 'checkpoint_during': True, 'durability': 'async'}, 'multitask_strategy': 'enqueue'}\n"
          ]
        }
      ],
      "source": [
        "# Check the run status\n",
        "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that it has `'status': 'pending'` because it is still running.\n",
        "\n",
        "What if we want to wait until the run completes, making it a blocking run?\n",
        "\n",
        "We can use `client.runs.join` to wait until the run completes.\n",
        "\n",
        "This ensures that no new runs are started until the current run completes on the thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'run_id': '019c69bb-2241-7522-a8a6-85a779715c18', 'thread_id': '6e808947-648c-4cf7-9c78-0e3e393104fe', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'created_at': '2026-02-17T03:53:17.634747+00:00', 'updated_at': '2026-02-17T03:53:20.789154+00:00', 'status': 'success', 'metadata': {'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}, 'kwargs': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Give me a summary of all ToDos.', 'additional_kwargs': {}, 'response_metadata': {}}]}, 'config': {'configurable': {'thread_id': '6e808947-648c-4cf7-9c78-0e3e393104fe', 'graph_id': 'task_maistro', '__request_start_time_ms__': 1771300397633, 'langgraph_request_id': '4fd0df50-4039-44dd-bf4f-51c0fb6002d8', 'langgraph_auth_user': None, 'user_id': 'Test', 'langgraph_auth_user_id': '', '__after_seconds__': 0, 'langgraph_auth_permissions': [], 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21', 'run_id': '019c69bb-2241-7522-a8a6-85a779715c18'}, 'metadata': {'created_by': 'system', 'assistant_id': 'ea4ebafa-a81d-5063-a5fa-67c755d98a21'}}, 'context': {'user_id': 'Test'}, 'command': None, 'stream_mode': ['values'], 'interrupt_before': None, 'interrupt_after': None, 'webhook': None, 'feedback_keys': None, 'temporary': False, 'subgraphs': False, 'resumable': False, 'checkpoint_during': True, 'durability': 'async'}, 'multitask_strategy': 'enqueue'}\n"
          ]
        }
      ],
      "source": [
        "# Wait until the run completes\n",
        "await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n",
        "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the run has `'status': 'success'` because it has completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can get run in the api docs as well.\n",
        "\n",
        "![get_run](../images/get_run.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Streaming Runs\n",
        "\n",
        "Each time a client makes a streaming request:\n",
        "\n",
        "1. The HTTP worker generates a unique run ID\n",
        "2. The Queue worker begins work on the run\n",
        "3. During execution, the Queue worker publishes updates to Redis\n",
        "4. The HTTP worker subscribes to those updates and streams them back to the client\n",
        "\n",
        "This is what enables token-by-token streaming over HTTP.\n",
        "\n",
        "We use `stream_mode=\"messages-tuple\"` to [stream tokens](https://docs.langchain.com/langsmith/streaming) — useful for production agents that may take a while to respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Since you have a task that is time-sensitive, I recommend focusing on the following:\n",
            "\n",
            "1. **Call parents back about Thanksgiving plans**\n",
            "   - This task has no deadline but is likely important to complete soon, especially if Thanksgiving is approaching.\n",
            "\n",
            "After that, you can prioritize the travel booking for Hong Kong, as it has a deadline of February 24, 2026. \n",
            "\n",
            "Would you like to mark any of these tasks as in progress or completed?"
          ]
        }
      ],
      "source": [
        "user_input = \"What ToDo should I focus on first.\"\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input)]},\n",
        "    config=config,\n",
        "    stream_mode=\"messages-tuple\",\n",
        "):\n",
        "    if chunk.event == \"messages\":\n",
        "        print(\n",
        "            \"\".join(data_item[\"content\"] for data_item in chunk.data if \"content\" in data_item),\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## Threads\n",
        "\n",
        "Whereas a run is only a single execution of the graph, a thread supports *multi-turn* interactions.\n",
        "\n",
        "When the client makes a graph execution execution with a `thread_id`, the server will save all [checkpoints](https://docs.langchain.com/oss/python/langgraph/persistence#checkpoints) (steps) in the run to the thread in the Postgres database.\n",
        "\n",
        "The server allows us <!-- to  [~check the status of created threads~](https://langchain-ai.github.io/langgraph/cloud/how-tos/check_thread_status/) -->\n",
        "a variety of ways to [work with threads](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient).\n",
        "\n",
        "### Check thread state\n",
        "\n",
        "For example, we can easily access the state [checkpoints](https://docs.langchain.com/oss/python/langgraph/persistence#checkpoints) saved to any specific thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give me a summary of all ToDos.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here’s a summary of your current ToDo list:\n",
            "\n",
            "1. **Task:** Call parents back about Thanksgiving plans\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** None\n",
            "   - **Time to Complete:** 30 minutes\n",
            "\n",
            "2. **Task:** Finish booking travel to Hong Kong\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** February 24, 2026\n",
            "   - **Solutions:** \n",
            "     - Check flight options\n",
            "     - Book accommodation\n",
            "     - Plan itinerary\n",
            "   - **Time to Complete:** 120 minutes\n",
            "\n",
            "3. **Task:** Finish booking travel to Hong Kong (duplicate entry)\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** February 24, 2026\n",
            "   - **Solutions:** \n",
            "     - Check flight options\n",
            "     - Book accommodation\n",
            "     - Plan itinerary\n",
            "   - **Time to Complete:** 120 minutes\n",
            "\n",
            "Let me know if you need any changes or updates!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What ToDo should I focus on first.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Since you have a task that is time-sensitive, I recommend focusing on the following:\n",
            "\n",
            "1. **Call parents back about Thanksgiving plans**\n",
            "   - This task has no deadline but is likely important to complete soon, especially if Thanksgiving is approaching.\n",
            "\n",
            "After that, you can prioritize the travel booking for Hong Kong, as it has a deadline of February 24, 2026. \n",
            "\n",
            "Would you like to mark any of these tasks as in progress or completed?\n"
          ]
        }
      ],
      "source": [
        "thread_state = await client.threads.get_state(thread['thread_id'])\n",
        "for m in convert_to_messages(thread_state['values']['messages']):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Copy threads\n",
        "\n",
        "We can also [copy](https://docs.langchain.com/langsmith/use-threads#copy-thread) (i.e. \"fork\") an existing thread. \n",
        "\n",
        "This will keep the existing thread's history, but allow us to create independent runs that do not affect the original thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Copy the thread\n",
        "copied_thread = await client.threads.copy(thread['thread_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give me a summary of all ToDos.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here’s a summary of your current ToDo list:\n",
            "\n",
            "1. **Task:** Call parents back about Thanksgiving plans\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** None\n",
            "   - **Time to Complete:** 30 minutes\n",
            "\n",
            "2. **Task:** Finish booking travel to Hong Kong\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** February 24, 2026\n",
            "   - **Solutions:** \n",
            "     - Check flight options\n",
            "     - Book accommodation\n",
            "     - Plan itinerary\n",
            "   - **Time to Complete:** 120 minutes\n",
            "\n",
            "3. **Task:** Finish booking travel to Hong Kong (duplicate entry)\n",
            "   - **Status:** Not started\n",
            "   - **Deadline:** February 24, 2026\n",
            "   - **Solutions:** \n",
            "     - Check flight options\n",
            "     - Book accommodation\n",
            "     - Plan itinerary\n",
            "   - **Time to Complete:** 120 minutes\n",
            "\n",
            "Let me know if you need any changes or updates!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What ToDo should I focus on first.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Since you have a task that is time-sensitive, I recommend focusing on the following:\n",
            "\n",
            "1. **Call parents back about Thanksgiving plans**\n",
            "   - This task has no deadline but is likely important to complete soon, especially if Thanksgiving is approaching.\n",
            "\n",
            "After that, you can prioritize the travel booking for Hong Kong, as it has a deadline of February 24, 2026. \n",
            "\n",
            "Would you like to mark any of these tasks as in progress or completed?\n"
          ]
        }
      ],
      "source": [
        "# Check the state of the copied thread\n",
        "copied_thread_state = await client.threads.get_state(copied_thread['thread_id'])\n",
        "for m in convert_to_messages(copied_thread_state['values']['messages']):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Human in the loop\n",
        "\n",
        "The server supports all [human-in-the-loop](https://docs.langchain.com/langsmith/add-human-in-the-loop) features — you can search, edit state, and resume graph execution from any prior checkpoint.\n",
        "\n",
        "As an example, [we can search, edit, and continue graph execution](https://docs.langchain.com/oss/python/langgraph/persistence#capabilities) from any prior checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'Give me a summary of all ToDos.',\n",
              "   'additional_kwargs': {},\n",
              "   'response_metadata': {},\n",
              "   'type': 'human',\n",
              "   'name': None,\n",
              "   'id': 'd08ed8d9-166e-42da-95ce-58604a79c07c'}]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the history of the thread\n",
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "\n",
        "# Pick a state update to fork\n",
        "to_fork = states[-2]\n",
        "to_fork['values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d08ed8d9-166e-42da-95ce-58604a79c07c'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['values']['messages'][0]['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['task_mAIstro']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['next']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1f10bb43-16d7-612e-8000-58aeaab7eb3b'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_fork['checkpoint_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's edit the state. Remember how our reducer on `messages` works: \n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "forked_input = {\n",
        "    \"messages\": HumanMessage(\n",
        "        content=\"Give me a summary of all ToDos that need to be done in the next week.\",\n",
        "        id=to_fork[\"values\"][\"messages\"][0][\"id\"],\n",
        "    )\n",
        "}\n",
        "\n",
        "# Update the state, creating a new checkpoint in the thread\n",
        "forked_config = await client.threads.update_state(\n",
        "    thread[\"thread_id\"], forked_input, checkpoint_id=to_fork[\"checkpoint_id\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Currently, your ToDo list includes the following tasks:\n",
            "\n",
            "1. **Call parents back about Thanksgiving plans**\n",
            "   - Status: Not started\n",
            "   - Deadline: None\n",
            "   - Time to complete: 30 minutes\n",
            "\n",
            "2. **Finish booking travel to Hong Kong**\n",
            "   - Status: Not started\n",
            "   - Deadline: February 24, 2026\n",
            "   - Solutions: Check flight options, Book accommodation, Plan itinerary\n",
            "   - Time to complete: 120 minutes\n",
            "\n",
            "None of these tasks have a deadline within the next week. The only task without a deadline is the call to your parents, which you can complete at your convenience. If you have any other tasks or deadlines in mind, feel free to let me know!"
          ]
        }
      ],
      "source": [
        "# Run the graph from the new checkpoint in the thread\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input=None,\n",
        "    config=config,\n",
        "    checkpoint_id=forked_config[\"checkpoint_id\"],\n",
        "    stream_mode=\"messages-tuple\",\n",
        "):\n",
        "    if chunk.event == \"messages\":\n",
        "        print(\n",
        "            \"\".join(data_item[\"content\"] for data_item in chunk.data if \"content\" in data_item),\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## Across-thread memory\n",
        "\n",
        "The `task_maistro` graph uses the [LangGraph memory `store`](https://docs.langchain.com/oss/python/langgraph/persistence#memory-store) to persist information across threads — such as the user's profile, ToDo list, and custom instructions.\n",
        "\n",
        "The deployment's Postgres database backs this store, making all memories durable across restarts.\n",
        "\n",
        "There are several methods [for interacting with the store](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.StoreClient) via the LangGraph SDK.\n",
        "\n",
        "### Search items\n",
        "\n",
        "`task_maistro` saves ToDos under the namespace tuple `(\"todo\", todo_category, user_id)`.\n",
        "\n",
        "The `todo_category` defaults to `\"general\"` (configurable in `configuration.py`).\n",
        "\n",
        "Supply this tuple to search for all ToDos for a given user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'namespace': ['todo', 'general', 'Test'],\n",
              "  'key': '149199ac-87af-4ab4-8c51-df5c24331aa8',\n",
              "  'value': {'task': 'Call parents back about Thanksgiving plans',\n",
              "   'status': 'not started',\n",
              "   'deadline': None,\n",
              "   'solutions': [],\n",
              "   'time_to_complete': 30},\n",
              "  'created_at': '2026-02-17T03:53:16.567869+00:00',\n",
              "  'updated_at': '2026-02-17T03:53:16.567869+00:00',\n",
              "  'score': None},\n",
              " {'namespace': ['todo', 'general', 'Test'],\n",
              "  'key': 'e4761eec-7bb0-4fe8-974b-a314aa34a0f6',\n",
              "  'value': {'task': 'Finish booking travel to Hong Kong',\n",
              "   'status': 'not started',\n",
              "   'deadline': '2026-02-24T00:00:00',\n",
              "   'solutions': ['Check flight options',\n",
              "    'Book accommodation',\n",
              "    'Plan itinerary'],\n",
              "   'time_to_complete': 120},\n",
              "  'created_at': '2026-02-17T03:53:16.567008+00:00',\n",
              "  'updated_at': '2026-02-17T03:53:16.567008+00:00',\n",
              "  'score': None},\n",
              " {'namespace': ['todo', 'general', 'Test'],\n",
              "  'key': '566063c5-4ac7-40b3-b582-7b21d6dfa5b8',\n",
              "  'value': {'task': 'Finish booking travel to Hong Kong',\n",
              "   'status': 'not started',\n",
              "   'deadline': '2026-02-24T00:00:00',\n",
              "   'solutions': ['Check flight options',\n",
              "    'Book accommodation',\n",
              "    'Plan itinerary'],\n",
              "   'time_to_complete': 120},\n",
              "  'created_at': '2026-02-17T03:53:12.535573+00:00',\n",
              "  'updated_at': '2026-02-17T03:53:12.535573+00:00',\n",
              "  'score': None}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"todo\", \"general\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also check in the interactive API docs.\n",
        "\n",
        "![search_store_items](../images/search_store_items.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Add items\n",
        "\n",
        "In our graph, we call `put` to add items to the store.\n",
        "\n",
        "We can use [put](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.StoreClient.put_item) with the SDK if we want to directly add items to the store outside our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "# # Put in same namespace\n",
        "# await client.store.put_item(\n",
        "#     (\"todo\", \"high\", \"Test\"),\n",
        "#     key=str(uuid4()),\n",
        "#     value={\"todo\": \"Test SDK put_item\"},\n",
        "# )\n",
        "\n",
        "# Put in different namespace\n",
        "await client.store.put_item(\n",
        "    (\"testing\", \"Test\"),\n",
        "    key=str(uuid4()),\n",
        "    value={\"todo\": \"Test SDK put_item\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'namespace': ['testing', 'Test'],\n",
              "  'key': '6284215f-abf3-435c-a124-de17ea811c19',\n",
              "  'value': {'todo': 'Test SDK put_item'},\n",
              "  'created_at': '2026-02-17T04:26:46.746637+00:00',\n",
              "  'updated_at': '2026-02-17T04:26:46.746637+00:00',\n",
              "  'score': None}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"testing\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "### Delete items\n",
        "\n",
        "We can use the SDK to [delete items](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.StoreClient.delete_item) from the store by key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['6284215f-abf3-435c-a124-de17ea811c19']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_keys = [item['key'] for item in items['items']]\n",
        "item_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "del_keys = item_keys[0]\n",
        "\n",
        "await client.store.delete_item(\n",
        "    (\"testing\", \"Test\"),\n",
        "    key=del_keys,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = await client.store.search_items(\n",
        "    (\"testing\", \"Test\"),\n",
        "    limit=5,\n",
        "    offset=0\n",
        ")\n",
        "items['items']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
